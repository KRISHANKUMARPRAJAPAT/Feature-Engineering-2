{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d82b2459-5ae5-412a-b731-61f9e26a2427",
   "metadata": {},
   "source": [
    "# Q1. What is the Filter method in feature selection, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f96f28-6e63-455d-a0e1-b592947edbc2",
   "metadata": {},
   "source": [
    "# Ans: 1 \n",
    "\n",
    "**Filter Method in Feature Selection:**\n",
    "\n",
    "The \"Filter method\" in feature selection is a technique used to select relevant features from a dataset based on their individual statistical properties, rather than relying on a machine learning model's performance. It is one of the simplest and most commonly used approaches for feature selection, especially in cases where the number of features is large compared to the number of samples.\n",
    "\n",
    "\n",
    "The general steps of the filter method are as follows:\n",
    "\n",
    "- **Calculate Feature Relevance Scores:** Each feature in the dataset is evaluated independently using some scoring metric that measures the relevance or importance of that feature with respect to the target variable (i.e., the variable we want to predict).\n",
    "- **Rank the Features:** The features are then ranked in descending order based on their relevance scores. The higher the score, the more important the feature is considered to be in relation to the target variable.\n",
    "- **Select the Top Features:** Finally, a fixed number of top-ranked features or features with relevance scores above a certain threshold are selected and retained for further analysis or model building.\n",
    "Common statistical measures used as feature relevance scores in the filter method include Pearson correlation coefficient, Chi-squared test (for categorical features), Information gain, and Mutual information.\n",
    "\n",
    "**Filter Method Work in Feature Selection**\n",
    "\n",
    "The Filter method works independently of any machine learning model. Here's how it works:\n",
    "\n",
    "- **Calculate Feature Relevance Scores** For each feature in the dataset, a statistical measure is computed to assess its relevance with respect to the target variable. The choice of the statistical measure depends on the data type of the feature (e.g., correlation for numerical features, chi-squared for categorical features).\n",
    "\n",
    "- **Rank the Features:** After calculating the relevance scores, the features are ranked in descending order based on their scores. The most relevant features will have higher scores, indicating their importance with respect to the target variable.\n",
    "\n",
    "- **Select the Top Features:** The filter method then selects a fixed number of top-ranked features or those with relevance scores above a predefined threshold. These selected features are retained for further analysis or model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2387e696-480c-4f15-a9f7-218a127335b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcc63430-1964-4ec1-9ac6-c71e788563f5",
   "metadata": {},
   "source": [
    "# Q2. How does the Wrapper method differ from the Filter method in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a6d18-5670-490d-9cff-b98361f80d26",
   "metadata": {},
   "source": [
    "# Ans: 2\n",
    "\n",
    "The Wrapper method and the Filter method are two different approaches to feature selection in machine learning. They both aim to select relevant features from a dataset, but they differ in their underlying principles and how they evaluate the importance of features.\n",
    "\n",
    "**Wrapper Method:**\n",
    "\n",
    "The Wrapper method selects features based on how well a given machine learning model performs when using a particular subset of features. It \n",
    "\n",
    "involves the following steps:\n",
    "\n",
    "- **Subset Generation:** The Wrapper method starts by trying out different combinations of features from the original feature set. It creates subsets of features and evaluates each subset's performance using a chosen machine learning algorithm.\n",
    "\n",
    "- **Model Training and Evaluation:** For each subset of features, a machine learning model is trained, and its performance is evaluated using a predefined evaluation metric, such as accuracy, precision, recall, or F1-score.\n",
    "\n",
    "- **Feature Selection:** The Wrapper method keeps track of the performance of each subset, and the subset that achieves the best performance (as per the chosen evaluation metric) is selected as the final set of features.\n",
    "\n",
    "**Advantages of Wrapper Method:**\n",
    "\n",
    "- Considers feature interactions: The Wrapper method can capture the interactions between features because it evaluates them in combination.\n",
    "- Optimizes model performance: By directly optimizing the model's performance, it aims to find the most relevant features for a specific machine learning task.\n",
    "\n",
    "**Disadvantages of Wrapper Method:**\n",
    "\n",
    "- Computationally expensive: As it needs to train and evaluate the model for each feature subset, the Wrapper method can be computationally intensive, especially with a large number of features.\n",
    "- Prone to overfitting: There is a risk of overfitting to the specific dataset used during the feature selection process, leading to suboptimal generalization.\n",
    "\n",
    ".\n",
    "\n",
    "**Filter Method:**\n",
    "\n",
    "The Filter method selects features based on their individual statistical properties and their relationship with the target variable, independent of any machine learning model.\n",
    "\n",
    "It involves the following steps:\n",
    "\n",
    "- **Calculate Feature Relevance Scores:** Each feature is evaluated independently using a statistical measure that quantifies its relevance or importance concerning the target variable.\n",
    "\n",
    "- **Rank the Features:** Features are ranked based on their relevance scores, with higher scores indicating higher importance.\n",
    "\n",
    "- **Feature Selection:** A fixed number of top-ranked features or features with relevance scores above a predefined threshold are selected and retained for further analysis or model building.\n",
    "\n",
    "**Advantages of Filter Method:**\n",
    "\n",
    "- Computationally efficient: The Filter method is computationally efficient since it does not involve training and evaluating machine learning models.\n",
    "- Less prone to overfitting: Since it evaluates features independently, it is less likely to overfit the specific dataset used during feature selection.\n",
    "\n",
    "**Disadvantages of Filter Method:**\n",
    "\n",
    "- Ignores feature interactions: The Filter method does not consider the interactions between features and may miss out on important feature combinations.\n",
    "- Suboptimal feature subset: The selected features might not lead to the best model performance, as it only evaluates them individually without considering their joint impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858409f-f556-4f39-b05c-d2051476ae0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a94b7eb8-0987-40a3-ade7-f73a65b4e52f",
   "metadata": {},
   "source": [
    "# Q3. What are some common techniques used in Embedded feature selection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41e8f83-5fde-4f66-898b-dac77efd041f",
   "metadata": {},
   "source": [
    "# Ans: 3 \n",
    "\n",
    "\n",
    "Embedded feature selection methods are techniques that perform feature selection as an integral part of the model training process. These methods aim to find the most relevant features while simultaneously optimizing the model's performance. Here are some common techniques used in \n",
    "\n",
    "Embedded feature selection methods:\n",
    "\n",
    "**L1 Regularization (Lasso Regression): L1 regularization adds a penalty term to the model's cost function proportional to the absolute values of the model's coefficients. This penalty encourages sparsity in the model by driving some coefficients to exactly zero. As a result, Lasso Regression can automatically perform feature selection by effectively setting the less important features' coefficients to zero.\n",
    "\n",
    "L2 Regularization (Ridge Regression): L2 regularization adds a penalty term to the model's cost function proportional to the squared magnitudes of the model's coefficients. While L2 regularization doesn't set coefficients to exactly zero like L1 regularization, it penalizes large coefficients, which can effectively downweight less important features.\n",
    "\n",
    "Elastic Net: Elastic Net combines L1 and L2 regularization to leverage the strengths of both techniques. It adds both penalty terms to the model's cost function, enabling feature selection and handling multicollinearity more effectively.\n",
    "\n",
    "Decision Trees (and Tree-based Ensembles): Decision trees inherently perform feature selection by choosing the most discriminative features to split the data at each node. In tree-based ensemble models like Random Forest and Gradient Boosting Machines (GBM), feature importance scores can be obtained, which rank the features based on their contribution to the model's predictive performance.\n",
    "\n",
    "Recursive Feature Elimination (RFE): RFE is an iterative technique commonly used with linear models or support vector machines. It starts with all features and removes the least important feature in each iteration until the desired number of features is reached or performance stabilizes.\n",
    "\n",
    "Gradient-Based Feature Importance: Some models, like Gradient Boosting Machines, provide feature importance scores based on how often a feature is used to make decisions during the boosting process. Features with higher importance scores are considered more relevant.\n",
    "\n",
    "Regularized Linear Models: Beyond L1 and L2 regularization, various regularized linear models, such as Elastic Net, Least Absolute Shrinkage and Selection Operator (LASSO), and Least-Angle Regression (LARS), are used for embedded feature selection.\n",
    "\n",
    "Sparse Autoencoders: Autoencoders can be used for unsupervised feature learning. By encouraging sparsity in the learned representations, some of the less important features can be effectively discarded.\n",
    "\n",
    "Genetic Algorithms: Genetic algorithms can be used to search for a subset of features that optimizes the model's performance by evaluating different feature combinations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb84fdc-7f56-40bb-a95f-1094abb00f70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e160c17f-f2bb-483c-8986-a7de2b944c6a",
   "metadata": {},
   "source": [
    "# Q4. What are some drawbacks of using the Filter method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f1a3a-2d63-487d-937d-0c9ff71e320c",
   "metadata": {},
   "source": [
    "# Ans: 4 \n",
    "\n",
    "\n",
    "While the Filter method has its advantages, it also comes with some drawbacks and limitations. Here are some of the drawbacks of using the \n",
    "\n",
    "Filter method for feature selection:\n",
    "\n",
    "Ignores Feature Interactions: The Filter method evaluates features individually based on their statistical properties and relevance to the target variable. It does not consider feature interactions or how the combination of features might contribute to the model's performance. Consequently, it may miss out on important patterns that involve multiple features working together.\n",
    "\n",
    "Doesn't Optimize Model Performance: Unlike Wrapper methods and Embedded methods, the Filter method does not directly optimize the model's performance metric. It selects features based solely on their individual relevance scores, which may not lead to the best-performing model for a given task.\n",
    "\n",
    "Limited to Feature Relevance Scores: The Filter method relies on a limited set of statistical measures (e.g., Pearson correlation, chi-squared, mutual information) to quantify feature relevance. These measures may not capture the full complexity and relevance of features in certain scenarios.\n",
    "\n",
    "Fixed Feature Selection: The Filter method typically selects a fixed number of top-ranked features or features with relevance scores above a predefined threshold. This fixed selection may not be the optimal subset of features for different machine learning algorithms or specific modeling tasks.\n",
    "\n",
    "Sensitive to Noise: The Filter method's performance can be sensitive to noise in the data. Noisy features with high variance but low relevance may still be selected if their relevance scores are high due to random fluctuations.\n",
    "\n",
    "Data Distribution Dependency: The performance of the Filter method heavily depends on the data distribution and the chosen statistical measures. Different datasets with varying characteristics may yield different feature subsets, leading to inconsistent results.\n",
    "\n",
    "Limited to Supervised Settings: The Filter method is most commonly used in supervised learning settings where there is a target variable. It might not be as effective in unsupervised learning tasks where the target variable is absent.\n",
    "\n",
    "Cannot Incorporate Domain Knowledge: The Filter method's decision on feature selection is solely based on statistical measures and does not allow incorporating domain knowledge or expert insights, which can be valuable in feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285ff0b3-8174-4ddb-886c-a16c70869b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b4d86c3-a496-4b25-b0cf-225f2f6f1b43",
   "metadata": {},
   "source": [
    "# Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e4924-d9f5-4300-82fe-c9e06d53972f",
   "metadata": {},
   "source": [
    "# Ans: 5 \n",
    "\n",
    "\n",
    "The choice between using the Filter method and the Wrapper method for feature selection depends on various factors, including the dataset characteristics, computational resources, and the specific goals of the analysis. \n",
    "\n",
    "There are situations where the Filter method might be more suitable:\n",
    "\n",
    "Large Datasets: The Filter method is computationally efficient since it does not involve training and evaluating a machine learning model for each feature subset. Therefore, when dealing with large datasets with a high number of features, the Filter method can be preferred due to its speed and scalability.\n",
    "\n",
    "Exploratory Data Analysis: If the primary goal is to gain insights into the data and identify potentially relevant features quickly, the Filter method can be a good choice. It provides a fast way to rank features based on their individual relevance scores, allowing for a rapid overview of feature importance.\n",
    "\n",
    "Highly Correlated Features: When dealing with highly correlated features, the Wrapper method might struggle to select the most relevant subset since correlated features can behave differently when combined. The Filter method, on the other hand, can effectively handle correlated features by assessing their individual relevance.\n",
    "\n",
    "Feature Engineering and Preprocessing: The Filter method can be useful during the feature engineering and preprocessing stages of a machine learning pipeline. It helps identify potential informative features early on and provides a starting point for more sophisticated feature selection techniques later.\n",
    "\n",
    "Baseline Feature Selection: The Filter method can be used as a baseline approach to establish a foundation for comparison with more complex methods. It can help assess the performance gains achieved by Wrapper methods or Embedded methods.\n",
    "\n",
    "No Access to Target Labels: In unsupervised or semi-supervised learning scenarios where the target labels are not available or are sparsely labeled, the Wrapper method is not directly applicable. In such cases, the Filter method can be used to perform feature selection based solely on the data distribution and feature characteristics.\n",
    "\n",
    "Model Interpretability: If interpretability is a critical requirement, the Filter method might be preferred as it selects features based on easily interpretable statistical measures rather than relying on model performance metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc5c3a-ed57-49be-8a4f-13a55537d462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18bcec57-e3d1-49a9-93a5-4a5decfb9a49",
   "metadata": {},
   "source": [
    "# Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec475e9-218a-40c1-9ba9-d2abfc9b7ba2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ans: 6 \n",
    "\n",
    "\n",
    "In the context of developing a predictive model for customer churn in a telecom company, the Filter Method is one of the techniques used for feature selection. The goal of the Filter Method is to assess the relevance of individual features by measuring their statistical properties and how they correlate with the target variable (churn in this case) without involving the predictive model.\n",
    "\n",
    "Here's a step-by-step guide on how to choose the most pertinent attributes for the model using the Filter Method:\n",
    "\n",
    "**1.Data Preparation:** First, ensure that the dataset is cleaned, preprocessed, and ready for analysis. Handle missing values, outliers, and other data quality issues.\n",
    "\n",
    "**2.Feature Scaling:** Perform feature scaling if necessary to normalize the attributes and bring them to a similar scale. This step is important, especially when dealing with distance-based metrics.\n",
    "\n",
    "**3.Feature Ranking:** Calculate the correlation or mutual information (depending on the data type) between each feature and the target variable (churn). Common methods for this include:\n",
    "\n",
    "- a. Pearson Correlation: For numerical features, compute the Pearson correlation coefficient between each feature and the target variable.\n",
    "\n",
    "- b. Cramér's V or Point-biserial correlation: For categorical features, calculate the Cramér's V or Point-biserial correlation with the target variable.\n",
    "\n",
    "- c. Mutual Information: Calculate the mutual information between each feature and the target variable. This is suitable for both numerical and categorical features.\n",
    "\n",
    "**4.Select Top Features:** Based on the calculated correlation or mutual information scores, rank the features in descending order. Select the top-k features with the highest scores. The value of k can be determined based on domain knowledge or using cross-validation techniques.\n",
    "\n",
    "**5.Remove Redundant Features:** Check for any high correlation or multicollinearity among the selected features. If two or more features are highly correlated, consider keeping only one of them to avoid redundancy.\n",
    "\n",
    "**6.Evaluate Model Performance:** Build a predictive model using only the selected features and evaluate its performance on a validation dataset using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score, ROC-AUC).\n",
    "\n",
    "**7.Fine-tuning:** Depending on the performance of the model, you can further fine-tune the feature selection process by adjusting the value of k, trying different correlation/mutual information metrics, or considering additional domain-specific knowledge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa5021-970e-48db-9e8a-146d6b6a22a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03c4374e-e58b-4873-b978-f6c25eb3e485",
   "metadata": {},
   "source": [
    "# Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210331bc-7707-4c2a-b7d3-52296e4338fe",
   "metadata": {},
   "source": [
    "# Ans: 7 \n",
    "\n",
    "\n",
    "In the context of predicting the outcome of a soccer match using a large dataset with many features, the Embedded method is a feature selection technique that combines feature selection with the model building process. The goal of the Embedded method is to identify the most relevant features while training the predictive model itself. This technique is particularly useful when dealing with high-dimensional datasets with potentially complex feature interactions.\n",
    "\n",
    "Here's how you can use the Embedded method to select the most relevant features for your soccer match outcome prediction model:\n",
    "\n",
    "**Data Preparation:** Start by cleaning and preprocessing the dataset to handle missing values, outliers, and any other data quality issues.\n",
    "\n",
    "**Feature Engineering:** Based on your domain knowledge and the dataset, create relevant features that might impact the outcome of a soccer match. For example, you could calculate team performance metrics, player-specific statistics, recent form, head-to-head results, and various other derived features.\n",
    "\n",
    "**Feature Scaling:** Perform feature scaling if necessary to normalize the attributes and bring them to a similar scale. This step is important, especially when using algorithms sensitive to feature scales, such as regularized models.\n",
    "\n",
    "**Model Selection:** Choose an appropriate machine learning algorithm for predicting the soccer match outcomes. Common choices include logistic regression, support vector machines, decision trees, random forests, gradient boosting, or neural networks.\n",
    "\n",
    "**Feature Selection with Regularization:** Many machine learning algorithms, such as Lasso regression and Ridge regression, incorporate regularization techniques. These techniques add penalty terms to the model's objective function, which discourages the model from relying heavily on less relevant features. The regularization process automatically selects and assigns higher weights to the most relevant features during model training.\n",
    "\n",
    "**Hyperparameter Tuning:** During the training process, tune the hyperparameters of the model and regularization strength to optimize performance and enhance feature selection. Use techniques like cross-validation to find the best hyperparameter values.\n",
    "\n",
    "**Evaluate Model Performance:** Once the model is trained, evaluate its performance on a validation dataset using appropriate evaluation metrics like accuracy, precision, recall, F1-score, or log-loss (depending on the nature of the problem).\n",
    "\n",
    "**Feature Importance Analysis:** After training the model, extract or analyze the feature importance scores provided by the model. These scores indicate the relative importance of each feature in predicting the soccer match outcomes. Based on these scores, you can identify the most relevant features for the final model.\n",
    "\n",
    "**Iterative Process:** If necessary, repeat the process of hyperparameter tuning and feature selection to fine-tune the model and improve its predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53df9b2-4195-4652-8599-0cff9db10304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "033d1038-be65-48a9-8b5c-5e15aa12bf70",
   "metadata": {},
   "source": [
    "# Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc189bb9-1341-413c-80fe-56b735b503c7",
   "metadata": {},
   "source": [
    " # Ans: 8 \n",
    "    \n",
    "\n",
    "In the context of predicting house prices based on a limited number of features, the Wrapper method is a feature selection technique that evaluates different subsets of features by training and testing the predictive model with each subset. The Wrapper method uses the performance of the model as a criterion for selecting the best set of features, making it an iterative and computationally expensive approach compared to filter methods. \n",
    "\n",
    "Here's how you can use the Wrapper method to select the best set of features for the predictor:\n",
    "\n",
    "**Data Preparation:** Start by cleaning and preprocessing the dataset to handle missing values, outliers, and any other data quality issues. Ensure that the data is in a suitable format for modeling.\n",
    "\n",
    "**Feature Engineering:** If needed, create additional features based on the existing ones that might improve the predictive power of the model. For example, you can calculate ratios, interactions, or polynomial features of the original features.\n",
    "\n",
    "**Train-Test Split:** Split the dataset into training and testing sets. The training set will be used for training the model, while the testing set will be used to evaluate the performance of the model with different subsets of features.\n",
    "\n",
    "**Select a Subset of Features:** Start with an initial subset of features (e.g., an empty set or a set containing a single feature). You will iteratively add and remove features to find the best combination.\n",
    "\n",
    "**Model Training and Evaluation:** Train a predictive model (e.g., linear regression, decision tree, random forest, etc.) using the selected subset of features from step 4. Evaluate the model's performance on the testing set using an appropriate evaluation metric, such as mean squared error (MSE) or mean absolute error (MAE) for regression tasks.\n",
    "\n",
    "**Feature Selection:** Based on the performance of the model in step 5, select the best-performing subset of features. You can use techniques like forward selection, backward elimination, or recursive feature elimination (RFE) to add or remove features from the current subset.\n",
    "\n",
    "vIterative Process:** Repeat steps 5 and 6 for different combinations of features until you find the best set of features that results in the optimal model performance.\n",
    "\n",
    "**Cross-Validation (Optional):** If the dataset size allows, you can further enhance the Wrapper method's reliability by using cross-validation during the feature selection process. This ensures that the model's performance is evaluated on different subsets of the data.\n",
    "\n",
    "**Final Model Training:** Once you have identified the best set of features using the Wrapper method, retrain the predictive model using the entire training dataset and the selected features.\n",
    "\n",
    "**Model Evaluation:** Evaluate the final model on the testing set to assess its performance on unseen data.\n",
    "\n",
    "Using the Wrapper method, you can systematically search for the most relevant set of features for predicting house prices. By iterating over different feature combinations and evaluating their impact on the model's performance, you can select the subset of features that yields the most accurate predictions. Keep in mind that the Wrapper method can be computationally intensive, especially for large datasets with a high number of features, so consider the computational resources available when applying this technique.   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f1ff2-6355-4f82-8764-7c2d30501a17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b9cdd-0da3-4d5e-9017-b92de38b3244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01913b6f-348c-493e-8f07-f1a427122821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b358dc2-45ce-4216-bcf1-996b2ada8bad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
